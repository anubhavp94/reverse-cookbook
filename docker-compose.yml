services:
  # Ollama service for AI model inference
  ollama:
    image: ollama/ollama:latest
    container_name: reverse-cookbook-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=http://localhost:3001,http://backend:3001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Backend service
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: reverse-cookbook-backend
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.1:8b
      - DB_PATH=/app/data/recipes.db
    volumes:
      - backend_data:/app/data
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Frontend service
  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: reverse-cookbook-frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:3001/api
    depends_on:
      - backend

  # Model setup service (runs once to pull the model)
  ollama-setup:
    image: curlimages/curl:latest
    container_name: reverse-cookbook-ollama-setup
    depends_on:
      - ollama
    command: |
      sh -c "
      echo 'Waiting for Ollama to be ready...' &&
      until curl -f http://ollama:11434/api/version; do
        echo 'Waiting for Ollama...'
        sleep 5
      done &&
      echo 'Pulling Llama 3.1 8B model...' &&
      curl -X POST http://ollama:11434/api/pull -H 'Content-Type: application/json' -d '{\"name\": \"llama3.1:8b\"}' &&
      echo 'Model setup complete!'
      "
    restart: "no"

volumes:
  ollama_data:
  backend_data: